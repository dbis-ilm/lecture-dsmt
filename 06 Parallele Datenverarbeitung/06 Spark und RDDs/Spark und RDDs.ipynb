{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b19bb56e-d971-4ff8-a470-7f04f86b4139",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# Spark und RDDs\n",
    "Apache Spark ist ein leistungsfähiges Open-Source-Framework für die Verarbeitung großer Datenmengen auf verteilten Systemen. Es ist für seine Vielseitigkeit - insbesondere im Vergleich zu MapReduce - und die Verwendung von Resilient Distributed Datasets (RDDs) bekannt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05d8d90-49e9-4106-a547-50ba93a79bf0",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "from tui_dsmt.parallel.datasets import text_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab85858a-d538-4a4e-be42-c888fd33aa69",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Inhaltsverzeichnis\n",
    "- [Apache Spark](#Apache-Spark)\n",
    "- [Resilient Distributed Datasets](#Resilient-Distributed-Datasets)\n",
    "- [Apache Spark und Python](#Apache-Spark-und-Python)\n",
    "- [Zusammenfassung](#Zusammenfassung)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9817843-e6d1-4c2c-a92c-3f9da3be6ed2",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Apache Spark\n",
    "Apache Spark ist ein Open-Source-System zur verteilten Verarbeitung großer Datenmengen. Die erste Version wurde 2014 veröffentlicht (und erst später an die Apache Software Foundation gespendet) und sollte Limitierungen älterer Verfahren überwinden. Der Ablauf von MapReduce ist beispielsweise sehr statisch: Daten werden (verteilt) gelesen, eine Map-Funktion wird auf sie angewandt, anschließend werden die aus der Map-Funktion resultierenden Schlüssel-Wert-Paare reduziert und wieder abgespeichert. MapReduce eignet sich daher ausschließlich für die Batch-Verarbeitung von Daten. Für Algorithmen, die zwangsweise mehrere Iterationen benötigen, wie das beispielsweise beim k-Means Verfahren notwendig ist, ist MapReduce ebenfalls wenig geeignet.\n",
    "\n",
    "Für Spark wurden daher einige grundsätzliche Designentscheidungen getroffen:\n",
    "- Die Verarbeitung kann In-Memory, also ohne Persistierung der Ergebnisse, stattfinden.\n",
    "- Es werden sowohl Batch- als auch Streaming-Daten unterstützt.\n",
    "- Eine Vielzahl von Programmiersprachen, darunter Python, können verwendet werden.\n",
    "- Umfangreiche Bibliotheken, z.B. für HDFS, SQL oder maschinelles Lernen, sollen die Benutzerfreundlichkeit erhöhen.\n",
    "- Resilient Distributed Datasets dienen als darunterliegende Datenstruktur, um Verteilbarkeit und Fehlertoleranz sicherzustellen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72cecd8a-85d3-4044-8d87-9dc636053590",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Resilient Distributed Datasets\n",
    "Resilient Distributed Datasets (RDDs) sind die zentrale Datenabstraktion in Apache Spark. Sie eröffnen die Möglichkeit einer einfachen, aber verteilten Ausführung verschiedener Transformations- und Aktionsoperationen auf großen Datenmengen. RDDs haben dazu einige Eigenschaften:\n",
    "1. RDDs sind in **Partitionen** unterteilt, die auf verschiedenen Systemen eines Clusters gespeichert werden, um Parallelität zu ermöglichen.\n",
    "2. **Transformationen** sind Operationen wie `map` oder `filter`, verändern also die Daten eines RDD. Transformationen werden grundsätzlich *lazy* angewandt (*lazy evaluation*).\n",
    "3. **Aktionen** sind Operationen wie `collect` oder `count`. Sie lösen tatsächlich Berechnungen aus und geben Ergebnisse zurück.\n",
    "4. RDDs sind **unveränderlich** (immutable). Jede Transformation eines RDD erzeugt ein neues, davon abgeleitetes RDD.\n",
    "5. Die Verknüpfungen von RDDs sind durch einen **gerichteten, azyklischen Graph** darstellbar.\n",
    "\n",
    "Die Vorteile der RDDs lassen sich somit sehr einfach zusammenfassen:\n",
    "- Durch die Partitionierung der RDDs liegen diese verteilt vor.\n",
    "- Durch Spark implementierte Transformationen und Aktionen nutzen diese Parallelität aus.\n",
    "- Ist ein RDD durch einen Ausfall eines Teilsystems nicht (mehr) verfügbar, kann es automatisch wiederhergestellt werden, indem die erforderlichen Teilschritte über den Abhängigkeitsgraphen gesucht und (erneut) ausgeführt werden."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295e8997-a6a5-49df-ac18-6935fc55b606",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "In Spark 2.x steht neben der RDD API auch die Dataset API zur Verfügung. Sie bietet einen High-Level-Zugriff auf die Daten, welcher leichter durch die Spark Engine optimiert werden kann. Auch wenn die Verwendung der Dataset API nahegelegt wird, gilt die RDD API dennoch noch nicht als veraltet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960a15ca-59fd-4c15-87f1-11f0e1746282",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Apache Spark und Python\n",
    "PySpark bietet die Leistungsfähigkeit von Apache Spark kombiniert mit der Einfachheit und Flexibilität von Python. Es unterstützt viele Spark-Features wie die Integration von SQL, eine Anbindung an Pandas und Machine Learning. Die Verwendung eines objektorientierten Programmiermodells bildet dabei die Beziehung der RDDs ab und erlaubt *lazy evaluation*. Auf Grund der [Anzahl an Operationen](https://spark.apache.org/docs/latest/api/python/index.html) sollen an dieser Stelle nur wenige dieser anhand des Word-Count Beispiel aufgegriffen und die generelle Verwendung von PySpark gezeigt werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b102c88-54d4-4df7-9d68-428134ba97da",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# Das Erstellen eines Spark-Kontexts ist per Kontext-\n",
    "# Manager möglich. 'local' gibt an, dass kein Cluster,\n",
    "# sondern ein einzelner Knoten zur Ausführung\n",
    "# verwendet werden soll.\n",
    "with SparkContext('local', 'word count') as sc:\n",
    "    # Lokale Dateien können mit Hilfe ihres Pfads\n",
    "    # geladen werden.\n",
    "    input_rdd = sc.textFile(','.join(text_paths))\n",
    "\n",
    "    # Ein Aufruf von collect \"sammelt\" das aktuelle\n",
    "    # Ergebnis. Beim textFile wäre das eine Liste\n",
    "    # bestehend aus den einzelnen Zeilen *aller*\n",
    "    # bereitgestellten Dateien.\n",
    "    #all_lines = input_rdd.collect()\n",
    "    #print(ww)\n",
    "\n",
    "    # Map wendet eine Funktion auf jedes Element\n",
    "    # einer Liste an und gibt damit eine Liste\n",
    "    # der selben Größe zurück. flatMap\n",
    "    # funktioniert analog, gibt aber eine flache\n",
    "    # Liste zurück, sodass jedes Element auf\n",
    "    # beliebig viele abgebildet werden und die\n",
    "    # Ergebnisliste damit eine abweichende\n",
    "    # Länge besitzen kann.\n",
    "    words_rdd = input_rdd.flatMap(lambda line: line.split(' '))\n",
    "\n",
    "    # Durch Split können leere Strings entstehen,\n",
    "    # die zunächst gefiltert werden.\n",
    "    filtered_words_rdd = words_rdd.filter(lambda word: word)\n",
    "\n",
    "    # Wie bei MapReduce werden die einzelnen\n",
    "    # Wörter jetzt auf Paare der Form\n",
    "    # (Wort, 1) abgebildet.\n",
    "    word_tuples_rdd = filtered_words_rdd.map(lambda word: (word, 1))\n",
    "\n",
    "    # Die Reduktionsfunktion kann verwendet werden,\n",
    "    # um Werte (hier immer 1) anhand ihres\n",
    "    # Schlüssels zusammenzufassen und anschließend\n",
    "    # zu aggregieren.\n",
    "    word_counts_rdd = word_tuples_rdd.reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "    # Zuletzt wird das Ergebnis des finalen RDD\n",
    "    # angefordert. Erst damit wird die Berechnung\n",
    "    # ausgelöst. Auskommentieren der nachfolgenden\n",
    "    # Zeile führt dazu, dass keinerlei Berechnung\n",
    "    # vorgenommen wird.\n",
    "    word_counts = word_counts_rdd.collect()\n",
    "\n",
    "for word, count in word_counts[:10]:\n",
    "    print(word, count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b58fcc0-93af-44d7-a90e-994d01a5168b",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Anmerkungen zum Beispiel:\n",
    "- Es ergibt sich ein linearer Ausführungsgraph. Durch die mehrfache Verwendung eines RDD könnten allerdings Zwischenergebnisse wiederverwendet werden und ein komplexerer Ausführungsgraph würde entstehen.\n",
    "- Allein die Verwendung der Spark-Funktionen reicht aus, um parallel zu rechnen: RDDs werden automatisch im Cluster partitioniert und Funktionen implizit verteilt berechnet.\n",
    "- Die Erzeugung nicht verwendeter RDDs ergibt keinen Performance-Nachteil, da deren Ergebnisse gar nicht erst berechnet werden.\n",
    "- Durch Anpassung des Kontexts lassen sich derartige Berechnungen lokal testen und anschließend relativ einfach auf einen Cluster übertragen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6100dac3-c3aa-4f2b-a20b-b0f83eb5c183",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Zusammenfassung\n",
    "Spark ist ein vielseitiges und effizientes Framework zur verteilten Berechnung. Das Konzept der RDDs erlaubt dabei eine verständliche und abstrakte Programmierung, während Implementierungen für verschiedene Programmiersprachen zur Verfügung stehen."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
